{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드값 고정\n",
    "\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU 장비 설정\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# 훈련, 검증, 테스트 데이터 경로 설정\n",
    "train_path = 'FakeImageDetection_Dataset/train'\n",
    "valid_path = 'FakeImageDetection_Dataset/valid'\n",
    "\n",
    "class PCACrop:\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert image to numpy array\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        # Flatten the image and apply PCA\n",
    "        img_flat = img_np.reshape(-1, 3)\n",
    "        pca = PCA(n_components=2)\n",
    "        img_pca = pca.fit_transform(img_flat)\n",
    "\n",
    "        # Find the region with maximum variance along the first principal component\n",
    "        idx_max_var = np.argmax(img_pca[:, 0])\n",
    "        x, y = idx_max_var % img_np.shape[1], idx_max_var // img_np.shape[1]\n",
    "\n",
    "        # Crop the region\n",
    "        crop = F.crop(img, y, x, self.output_size[1], self.output_size[0])\n",
    "        return crop\n",
    "    \n",
    "class ResizeIfNeeded:\n",
    "    def __call__(self, image):\n",
    "        # 이미지의 가로, 세로 길이를 확인\n",
    "        width, height = image.size\n",
    "        \n",
    "        # 가로 또는 세로 길이가 224 미만인 경우를 확인하여 크기를 늘림\n",
    "        new_width = width if width >= 224 else 224\n",
    "        new_height = height if height >= 224 else 224\n",
    "        \n",
    "        # 이미지 크기를 변경\n",
    "        resized_image = image.resize((new_width, new_height), Image.BICUBIC)\n",
    "        return resized_image\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(ResizeIfNeeded()),\n",
    "    PCACrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 훈련 데이터셋\n",
    "datasets_train = ImageFolder(train_path, transform=transform)\n",
    "# 검증 데이터셋\n",
    "datasets_valid = ImageFolder(valid_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22c95d41cf0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# 제너레이터 시드값 고정\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "loader_train = DataLoader(dataset=datasets_train, batch_size=batch_size, \n",
    "                          shuffle=True, worker_init_fn=seed_worker,\n",
    "                          generator=g, num_workers=0)\n",
    "loader_valid = DataLoader(dataset=datasets_valid, batch_size=batch_size, \n",
    "                          shuffle=False, worker_init_fn=seed_worker,\n",
    "                          generator=g, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "# 모델 생성\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=2) \n",
    "# 장비 할당\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguddnjs2366\u001b[0m (\u001b[33mseohw\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user\\Desktop\\Workspace\\Thesis\\generative_image_detection\\Personal Research\\Resarch 1\\wandb\\run-20231106_004723-p9j644di</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seohw/FakeImageDetection_Research/runs/p9j644di' target=\"_blank\">Research1_4</a></strong> to <a href='https://wandb.ai/seohw/FakeImageDetection_Research' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seohw/FakeImageDetection_Research' target=\"_blank\">https://wandb.ai/seohw/FakeImageDetection_Research</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seohw/FakeImageDetection_Research/runs/p9j644di' target=\"_blank\">https://wandb.ai/seohw/FakeImageDetection_Research/runs/p9j644di</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/seohw/FakeImageDetection_Research/runs/p9j644di?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x22cab4d3a00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# wandb 초기화: 새로운 실험을 생성하거나 기존 실험에 연결\n",
    "wandb.init(project=\"FakeImageDetection_Research\", name=\"Research1_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score # 정확도 계산 함수\n",
    "from sklearn.metrics import recall_score   # 재현율 계산 함수\n",
    "from sklearn.metrics import f1_score       # F1 점수 계산 함수\n",
    "from tqdm.notebook import tqdm             # 진행률 표시 막대\n",
    "\n",
    "def train(model, loader_train, loader_valid, criterion, optimizer, \n",
    "          scheduler=None, epochs=10, save_file='model_state_dict.pth'):\n",
    "    \n",
    "    valid_loss_min = np.inf # 최소 손실값 초기화 (검증 데이터용) \n",
    "\n",
    "    # 총 에폭만큼 반복\n",
    "    for epoch in range(epochs):\n",
    "        print(f'에폭 [{epoch+1}/{epochs}] \\n-----------------------------')\n",
    "        \n",
    "        # == [ 훈련 ] ==============================================\n",
    "        model.train()        # 모델을 훈련 상태로 설정\n",
    "        epoch_train_loss = 0 # 에폭별 손실값 초기화 (훈련 데이터용)\n",
    "        train_preds_list = []  # 훈련 데이터 예측값 저장용 리스트 초기화\n",
    "        train_true_list = []   # 훈련 데이터 실젯값 저장용 리스트 초기화\n",
    "        # '반복 횟수'만큼 반복 \n",
    "        for images, labels in tqdm(loader_train):\n",
    "            # 이미지, 레이블(타깃값) 데이터 미니배치를 장비에 할당 \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # 옵티마이저 내 기울기 초기화\n",
    "            optimizer.zero_grad()\n",
    "            # 순전파 : 이미지 데이터를 신경망 모델의 입력값으로 사용해 출력값 계산\n",
    "            outputs = model(images)\n",
    "            # 손실 함수를 활용해 outputs와 labels의 손실값 계산\n",
    "            loss = criterion(outputs, labels)\n",
    "            # 현재 배치에서의 손실 추가 (훈련 데이터용)\n",
    "            epoch_train_loss += loss.item() \n",
    "            loss.backward()       # 역전파 수행\n",
    "            optimizer.step()      # 가중치 갱신\n",
    "            if scheduler != None: # 스케줄러 학습률 갱신 \n",
    "                scheduler.step() \n",
    "                \n",
    "            # 예측값 및 실제값 저장 (훈련 데이터용)\n",
    "            train_preds = torch.max(outputs.cpu(), dim=1)[1].numpy()\n",
    "            train_true = labels.cpu().numpy()\n",
    "            train_preds_list.extend(train_preds)\n",
    "            train_true_list.extend(train_true)\n",
    "            \n",
    "        # 훈련 데이터 정확도 계산\n",
    "        train_accuracy = accuracy_score(train_true_list, train_preds_list)\n",
    "\n",
    "        # 훈련 데이터 손실값 출력\n",
    "        print(f'\\t훈련 데이터 손실값 : {epoch_train_loss/len(loader_train):.4f}')\n",
    "        print(f'\\t훈련 데이터 정확도 : {train_accuracy:.4f}')\n",
    "        \n",
    "        # == [ 검증 ] ==============================================\n",
    "        model.eval()         # 모델을 평가 상태로 설정 \n",
    "        epoch_valid_loss = 0 # 에폭별 손실값 초기화 (검증 데이터용)\n",
    "        preds_list = []      # 예측값 저장용 리스트 초기화\n",
    "        true_list = []       # 실젯값 저장용 리스트 초기화\n",
    "        \n",
    "        with torch.no_grad(): # 기울기 계산 비활성화\n",
    "            for images, labels in loader_valid:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_valid_loss += loss.item()\n",
    "                \n",
    "                # 예측값 및 실제값 \n",
    "                preds = torch.max(outputs.cpu(), dim=1)[1].numpy() \n",
    "                true = labels.cpu().numpy() \n",
    "    \n",
    "                preds_list.extend(preds)\n",
    "                true_list.extend(true)\n",
    "                \n",
    "        # 정확도, 재현율, F1 점수 계산\n",
    "        val_accuracy = accuracy_score(true_list, preds_list)\n",
    "        val_recall = recall_score(true_list, preds_list)\n",
    "        val_f1_score = f1_score(true_list, preds_list)\n",
    "\n",
    "        wandb.log({\"train_loss\": epoch_train_loss/len(loader_train),\n",
    "                   \"train_accuracy\" : train_accuracy,\n",
    "                   \"val_loss\": epoch_valid_loss/len(loader_valid), \n",
    "                   \"val_accuracy\": val_accuracy,\n",
    "                   \"val_recall\" : val_recall,\n",
    "                   \"val_f1_score\" : val_f1_score})\n",
    "        \n",
    "        # 검증 데이터 손실값 및 정확도, 재현율, F1점수 출력\n",
    "        print(f'\\t검증 데이터 손실값 : {epoch_valid_loss/len(loader_valid):.4f}')\n",
    "        print(f'\\t정확도 : {val_accuracy:.4f} / 재현율 : {val_recall:.4f} / F1 점수 : {val_f1_score:.4f}')\n",
    "        # == [ 최적 모델 가중치 찾기 ] ==============================\n",
    "        # 현 에폭에서의 손실값이 최소 손실값 이하면 모델 가중치 저장 \n",
    "        if epoch_valid_loss <= valid_loss_min: \n",
    "            print(f'\\t### 검증 데이터 손실값 감소 ({valid_loss_min:.4f} --> {epoch_valid_loss:.4f}). 모델 저장')\n",
    "            # 모델 가중치를 파일로 저장 \n",
    "            torch.save(model.state_dict(), save_file) \n",
    "            valid_loss_min = epoch_valid_loss # 최소 손실값 갱신 \n",
    "    return torch.load(save_file) # 저장한 모델 가중치를 불러와 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에폭 [1/10] \n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700708086454456592f96e283a891127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t훈련 데이터 손실값 : 0.1585\n",
      "\t훈련 데이터 정확도 : 0.9284\n",
      "\t검증 데이터 손실값 : 0.1365\n",
      "\t정확도 : 0.9447 / 재현율 : 0.9670 / F1 점수 : 0.9460\n",
      "\t### 검증 데이터 손실값 감소 (inf --> 8.6008). 모델 저장\n",
      "에폭 [2/10] \n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456829d2c2af4c74a24bb38a115549c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t훈련 데이터 손실값 : 0.0741\n",
      "\t훈련 데이터 정확도 : 0.9651\n",
      "\t검증 데이터 손실값 : 0.1822\n",
      "\t정확도 : 0.9490 / 재현율 : 0.9695 / F1 점수 : 0.9500\n",
      "에폭 [3/10] \n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c03108e8474fdcad9951a3cb3819a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\Workspace\\Thesis\\generative_image_detection\\Personal Research\\Resarch 1\\EfficientnetTrain_Research1_4.ipynb 셀 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 모델 훈련\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_state_dict \u001b[39m=\u001b[39m train(model \u001b[39m=\u001b[39;49m model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                          loader_train \u001b[39m=\u001b[39;49m loader_train, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                          loader_valid \u001b[39m=\u001b[39;49m loader_valid,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                          criterion \u001b[39m=\u001b[39;49m criterion, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                          optimizer \u001b[39m=\u001b[39;49m optimizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\Workspace\\Thesis\\generative_image_detection\\Personal Research\\Resarch 1\\EfficientnetTrain_Research1_4.ipynb 셀 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m train_true_list \u001b[39m=\u001b[39m []   \u001b[39m# 훈련 데이터 실젯값 저장용 리스트 초기화\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# '반복 횟수'만큼 반복 \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m tqdm(loader_train):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# 이미지, 레이블(타깃값) 데이터 미니배치를 장비에 할당 \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\tqdm\\notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    255\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    257\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\torchvision\\datasets\\folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(sample)\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\Workspace\\Thesis\\generative_image_detection\\Personal Research\\Resarch 1\\EfficientnetTrain_Research1_4.ipynb 셀 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m img_flat \u001b[39m=\u001b[39m img_np\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m img_pca \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(img_flat)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Find the region with maximum variance along the first principal component\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/EfficientnetTrain_Research1_4.ipynb#X32sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m idx_max_var \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(img_pca[:, \u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:460\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    439\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m     U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m    461\u001b[0m     U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[0;32m    463\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[0;32m    464\u001b[0m         \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:512\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_full(X, n_components)\n\u001b[0;32m    511\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_truncated(X, n_components, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_svd_solver)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:616\u001b[0m, in \u001b[0;36mPCA._fit_truncated\u001b[1;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[0;32m    612\u001b[0m     U, Vt \u001b[39m=\u001b[39m svd_flip(U[:, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], Vt[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m    614\u001b[0m \u001b[39melif\u001b[39;00m svd_solver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    615\u001b[0m     \u001b[39m# sign flipping is done inside\u001b[39;00m\n\u001b[1;32m--> 616\u001b[0m     U, S, Vt \u001b[39m=\u001b[39m randomized_svd(\n\u001b[0;32m    617\u001b[0m         X,\n\u001b[0;32m    618\u001b[0m         n_components\u001b[39m=\u001b[39;49mn_components,\n\u001b[0;32m    619\u001b[0m         n_oversamples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_oversamples,\n\u001b[0;32m    620\u001b[0m         n_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterated_power,\n\u001b[0;32m    621\u001b[0m         power_iteration_normalizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpower_iteration_normalizer,\n\u001b[0;32m    622\u001b[0m         flip_sign\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    623\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[0;32m    626\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_ \u001b[39m=\u001b[39m n_samples\n\u001b[0;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents_ \u001b[39m=\u001b[39m Vt\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\sklearn\\utils\\extmath.py:469\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m flip_sign:\n\u001b[0;32m    468\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m transpose:\n\u001b[1;32m--> 469\u001b[0m         U, Vt \u001b[39m=\u001b[39m svd_flip(U, Vt)\n\u001b[0;32m    470\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    471\u001b[0m         \u001b[39m# In case of transpose u_based_decision=false\u001b[39;00m\n\u001b[0;32m    472\u001b[0m         \u001b[39m# to actually flip based on u and not v.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m         U, Vt \u001b[39m=\u001b[39m svd_flip(U, Vt, u_based_decision\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\sklearn\\utils\\extmath.py:805\u001b[0m, in \u001b[0;36msvd_flip\u001b[1;34m(u, v, u_based_decision)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Sign correction to ensure deterministic output from SVD.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \n\u001b[0;32m    773\u001b[0m \u001b[39mAdjusts the columns of u and the rows of v such that the loadings in the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[39m    Array v with adjusted rows and the same dimensions as v.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[39mif\u001b[39;00m u_based_decision:\n\u001b[0;32m    804\u001b[0m     \u001b[39m# columns of u, rows of v\u001b[39;00m\n\u001b[1;32m--> 805\u001b[0m     max_abs_cols \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(np\u001b[39m.\u001b[39;49mabs(u), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    806\u001b[0m     signs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msign(u[max_abs_cols, \u001b[39mrange\u001b[39m(u\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])])\n\u001b[0;32m    807\u001b[0m     u \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m signs\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margmax\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model_state_dict = train(model = model,\n",
    "                         loader_train = loader_train, \n",
    "                         loader_valid = loader_valid,\n",
    "                         criterion = criterion, \n",
    "                         optimizer = optimizer)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 가중치 불러오기\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datasets_test = ImageFolder('test_data', transform=transform)\n",
    "\n",
    "batch_sise = 256\n",
    "\n",
    "loader_test = DataLoader(dataset=datasets_test, batch_size=batch_size, \n",
    "                         shuffle=False, worker_init_fn=seed_worker,\n",
    "                         generator=g, num_workers=0)\n",
    "model.eval()  # 평가 모드로 전환\n",
    "\n",
    "# 3. 예측하기\n",
    "predictions = []\n",
    "\n",
    "for images, _ in loader_test:  # 레이블은 사용하지 않음\n",
    "    images = images.to(device)  # 장비 할당\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        prediction = outputs.argmax(dim=1).cpu().numpy()\n",
    "        predictions.extend(prediction)\n",
    "\n",
    "# 4. 결과 저장\n",
    "df = pd.read_csv('./test_data/sample_submission.csv')\n",
    "df['answer'] = predictions\n",
    "df['answer'] = 1 - df['answer']\n",
    "df.to_csv('./test_data/research1_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
