{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.fft import fft2, fftshift\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(img, size):\n",
    "    width, height = img.size   # 기존 이미지의 너비와 높이를 얻습니다.\n",
    "    new_width, new_height = size\n",
    "    if width < new_width or height < new_height:\n",
    "        # 이미지가 충분히 크지 않은 경우에는 사이즈를 늘려줍니다.\n",
    "        img = img.resize((max(new_width, width), max(new_height, height)))\n",
    "        width, height = img.size  # 사이즈를 늘린 후의 너비와 높이를 다시 얻습니다.\n",
    "    \n",
    "    left = random.randint(0, width - new_width)\n",
    "    top = random.randint(0, height - new_height)\n",
    "    right = left + new_width\n",
    "    bottom = top + new_height\n",
    "\n",
    "    return img.crop((left, top, right, bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path, pca):\n",
    "    # 이미지를 불러옵니다.\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    \n",
    "    # 이미지를 랜덤 크롭합니다.\n",
    "    image = random_crop(image, (128, 128))\n",
    "    \n",
    "    # 이미지 크기를 재조정합니다.\n",
    "    image = image.resize((64, 64))\n",
    "    image = np.array(image)\n",
    "\n",
    "    # 푸리에 변환을 수행합니다.\n",
    "    f_transform = fft2(image)\n",
    "    f_shift = fftshift(f_transform)\n",
    "    magnitude_spectrum = 20 * np.log(np.abs(f_shift))\n",
    "\n",
    "    # 평탄화(flatten)하여 1차원 배열로 만듭니다.\n",
    "    flattened_spectrum = magnitude_spectrum.flatten().reshape(1, -1)\n",
    "\n",
    "    # PCA를 적용하여 차원을 축소합니다. (여기서는 fit_transform 대신 transform을 사용해야 합니다.)\n",
    "    pca_features = pca.transform(flattened_spectrum)\n",
    "\n",
    "    return pca_features.flatten()  # 이제 2차원이 아닌 1차원 배열로 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This PCA instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\Workspace\\Thesis\\generative_image_detection\\Personal Research\\Resarch 1\\ML_Train.ipynb 셀 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m valid_labels \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(valid_fake_images) \u001b[39m+\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(valid_real_images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# 모든 학습 이미지에 대한 피처를 추출합니다.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_features \u001b[39m=\u001b[39m [extract_features(img, pca) \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m train_fake_images \u001b[39m+\u001b[39m train_real_images]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# 모든 검증 이미지에 대한 피처를 추출합니다.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m valid_features \u001b[39m=\u001b[39m [extract_features(img, pca) \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m valid_fake_images \u001b[39m+\u001b[39m valid_real_images]\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\Workspace\\Thesis\\generative_image_detection\\Personal Research\\Resarch 1\\ML_Train.ipynb 셀 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m valid_labels \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(valid_fake_images) \u001b[39m+\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(valid_real_images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# 모든 학습 이미지에 대한 피처를 추출합니다.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_features \u001b[39m=\u001b[39m [extract_features(img, pca) \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m train_fake_images \u001b[39m+\u001b[39m train_real_images]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# 모든 검증 이미지에 대한 피처를 추출합니다.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m valid_features \u001b[39m=\u001b[39m [extract_features(img, pca) \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m valid_fake_images \u001b[39m+\u001b[39m valid_real_images]\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\Workspace\\Thesis\\generative_image_detection\\Personal Research\\Resarch 1\\ML_Train.ipynb 셀 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m flattened_spectrum \u001b[39m=\u001b[39m magnitude_spectrum\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# PCA를 적용하여 차원을 축소합니다. (여기서는 fit_transform 대신 transform을 사용해야 합니다.)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m pca_features \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mtransform(flattened_spectrum)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Workspace/Thesis/generative_image_detection/Personal%20Research/Resarch%201/ML_Train.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pca_features\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\sklearn\\decomposition\\_base.py:119\u001b[0m, in \u001b[0;36m_BasePCA.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    102\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply dimensionality reduction to X.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[0;32m    104\u001b[0m \u001b[39m    X is projected on the first principal components previously extracted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m        is the number of samples and `n_components` is the number of the components.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    121\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(X, dtype\u001b[39m=\u001b[39m[np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32], reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    122\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch2\\lib\\site-packages\\sklearn\\utils\\validation.py:1462\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not an estimator instance.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (estimator))\n\u001b[0;32m   1461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1462\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This PCA instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# PCA 객체 초기화, n_components는 필요한 주성분 수에 따라 조정합니다.\n",
    "pca = PCA(n_components=50)\n",
    "\n",
    "# 학습 데이터\n",
    "train_fake_images = ['FakeImageDetection_Dataset/train/fake_images/' + f for f in os.listdir('FakeImageDetection_Dataset/train/fake_images')]\n",
    "train_real_images = ['FakeImageDetection_Dataset/train/real_images/' + f for f in os.listdir('FakeImageDetection_Dataset/train/real_images')]\n",
    "\n",
    "# 검증 데이터\n",
    "valid_fake_images = ['FakeImageDetection_Dataset/valid/valid_fake_images/' + f for f in os.listdir('FakeImageDetection_Dataset/valid/valid_fake_images')]\n",
    "valid_real_images = ['FakeImageDetection_Dataset/valid/valid_real_images/' + f for f in os.listdir('FakeImageDetection_Dataset/valid/valid_real_images')]\n",
    "\n",
    "# 레이블 생성: 가짜 이미지는 1, 진짜 이미지는 0\n",
    "train_labels = [1] * len(train_fake_images) + [0] * len(train_real_images)\n",
    "valid_labels = [1] * len(valid_fake_images) + [0] * len(valid_real_images)\n",
    "\n",
    "# 모든 학습 이미지에 대한 피처를 추출합니다.\n",
    "train_features = [extract_features(img, pca) for img in train_fake_images + train_real_images]\n",
    "\n",
    "# 모든 검증 이미지에 대한 피처를 추출합니다.\n",
    "valid_features = [extract_features(img, pca) for img in valid_fake_images + valid_real_images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMatrix 형태로 데이터를 변환합니다.\n",
    "dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
    "dvalid = xgb.DMatrix(valid_features, label=valid_labels)\n",
    "\n",
    "# 파라미터 설정\n",
    "params = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "# 학습\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = ['test_data/images/' + f for f in os.listdir('test_data/images')]\n",
    "test_features = [extract_features(img, pca) for img in test_images]\n",
    "\n",
    "dtest = xgb.DMatrix(test_features)\n",
    "predictions = bst.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, prediction in zip(test_images, predictions):\n",
    "    print(f'{img}: {\"Fake\" if prediction < 0.5 else \"Real\"}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
